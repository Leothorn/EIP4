{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ResNet v1, v2, and segmentation models for Keras.\n",
    "# Reference\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "- [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)\n",
    "Reference material for extended functionality:\n",
    "- [ResNeXt](https://arxiv.org/abs/1611.05431) for Tiny ImageNet support.\n",
    "- [Dilated Residual Networks](https://arxiv.org/pdf/1705.09914) for segmentation support\n",
    "- [Deep Residual Learning for Instrument Segmentation in\n",
    "   Robotic Surgery](https://arxiv.org/abs/1703.08580)\n",
    "  for segmentation support.\n",
    "Implementation Adapted from: github.com/raghakot/keras-resnet\n",
    "\"\"\"  # pylint: disable=E501\n",
    "from __future__ import division\n",
    "import keras\n",
    "import six\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D,SeparableConv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def _bn_relu(x, bn_name=None, relu_name=None):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS, name=bn_name)(x)\n",
    "    return Activation(\"relu\", name=relu_name)(norm)\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu residual unit activation function.\n",
    "       This is the original ResNet v1 scheme in https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    dilation_rate = conv_params.setdefault(\"dilation_rate\", (1, 1))\n",
    "    conv_name = conv_params.setdefault(\"conv_name\", None)\n",
    "    bn_name = conv_params.setdefault(\"bn_name\", None)\n",
    "    relu_name = conv_params.setdefault(\"relu_name\", None)\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(x):\n",
    "        x = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                   strides=strides, padding=padding,\n",
    "                   dilation_rate=dilation_rate,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=kernel_regularizer,\n",
    "                   name=conv_name)(x)\n",
    "        return _bn_relu(x, bn_name=bn_name, relu_name=relu_name)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv residual unit with full pre-activation\n",
    "    function. This is the ResNet v2 scheme proposed in\n",
    "    http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    dilation_rate = conv_params.setdefault(\"dilation_rate\", (1, 1))\n",
    "    conv_name = conv_params.setdefault(\"conv_name\", None)\n",
    "    bn_name = conv_params.setdefault(\"bn_name\", None)\n",
    "    relu_name = conv_params.setdefault(\"relu_name\", None)\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(x):\n",
    "        activation = _bn_relu(x, bn_name=bn_name, relu_name=relu_name)\n",
    "        return SeparableConv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      dilation_rate=dilation_rate,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer,\n",
    "                      name=conv_name)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input_feature, residual, conv_name_base=None, bn_name_base=None):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input_feature)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input_feature\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        print('reshaping via a convolution...')\n",
    "        if conv_name_base is not None:\n",
    "            conv_name_base = conv_name_base + '1'\n",
    "        shortcut = SeparableConv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001),\n",
    "                          name=conv_name_base)(input_feature)\n",
    "        if bn_name_base is not None:\n",
    "            bn_name_base = bn_name_base + '1'\n",
    "        shortcut = BatchNormalization(axis=CHANNEL_AXIS,\n",
    "                                      name=bn_name_base)(shortcut)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def _residual_block(block_function, filters, blocks, stage,\n",
    "                    transition_strides=None, transition_dilation_rates=None,\n",
    "                    dilation_rates=None, is_first_layer=False, dropout=None,\n",
    "                    residual_unit=_bn_relu_conv):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "       stage: integer, current stage label, used for generating layer names\n",
    "       blocks: number of blocks 'a','b'..., current block label, used for generating\n",
    "            layer names\n",
    "       transition_strides: a list of tuples for the strides of each transition\n",
    "       transition_dilation_rates: a list of tuples for the dilation rate of each\n",
    "            transition\n",
    "    \"\"\"\n",
    "    if transition_dilation_rates is None:\n",
    "        transition_dilation_rates = [(1, 1)] * blocks\n",
    "    if transition_strides is None:\n",
    "        transition_strides = [(1, 1)] * blocks\n",
    "    if dilation_rates is None:\n",
    "        dilation_rates = [1] * blocks\n",
    "\n",
    "    def f(x):\n",
    "        for i in range(blocks):\n",
    "            is_first_block = is_first_layer and i == 0\n",
    "            x = block_function(filters=filters, stage=stage, block=i,\n",
    "                               transition_strides=transition_strides[i],\n",
    "                               dilation_rate=dilation_rates[i],\n",
    "                               is_first_block_of_first_layer=is_first_block,\n",
    "                               dropout=dropout,\n",
    "                               residual_unit=residual_unit)(x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _block_name_base(stage, block):\n",
    "    \"\"\"Get the convolution name base and batch normalization name base defined by\n",
    "    stage and block.\n",
    "    If there are less than 26 blocks they will be labeled 'a', 'b', 'c' to match the\n",
    "    paper and keras and beyond 26 blocks they will simply be numbered.\n",
    "    \"\"\"\n",
    "    if block < 27:\n",
    "        block = '%c' % (block + 97)  # 97 is the ascii number for lowercase 'a'\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    return conv_name_base, bn_name_base\n",
    "\n",
    "\n",
    "def basic_block(filters, stage, block, transition_strides=(1, 1),\n",
    "                dilation_rate=(1, 1), is_first_block_of_first_layer=False, dropout=None,\n",
    "                residual_unit=_bn_relu_conv):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    def f(input_features):\n",
    "        conv_name_base, bn_name_base = _block_name_base(stage, block)\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            x = SeparableConv2D(filters=filters, kernel_size=(3, 3),\n",
    "                       strides=transition_strides,\n",
    "                       dilation_rate=dilation_rate,\n",
    "                       padding=\"same\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2(1e-4),\n",
    "                       name=conv_name_base + '2a')(input_features)\n",
    "        else:\n",
    "            x = residual_unit(filters=filters, kernel_size=(3, 3),\n",
    "                              strides=transition_strides,\n",
    "                              dilation_rate=dilation_rate,\n",
    "                              conv_name_base=conv_name_base + '2a',\n",
    "                              bn_name_base=bn_name_base + '2a')(input_features)\n",
    "\n",
    "        if dropout is not None:\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "        x = residual_unit(filters=filters, kernel_size=(3, 3),\n",
    "                          conv_name_base=conv_name_base + '2b',\n",
    "                          bn_name_base=bn_name_base + '2b')(x)\n",
    "\n",
    "        return _shortcut(input_features, x)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck(filters, stage, block, transition_strides=(1, 1),\n",
    "               dilation_rate=(1, 1), is_first_block_of_first_layer=False, dropout=None,\n",
    "               residual_unit=_bn_relu_conv):\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "    def f(input_feature):\n",
    "        conv_name_base, bn_name_base = _block_name_base(stage, block)\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            x = SeparableConv2D(filters=filters, kernel_size=(1, 1),\n",
    "                       strides=transition_strides,\n",
    "                       dilation_rate=dilation_rate,\n",
    "                       padding=\"same\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2(1e-4),\n",
    "                       name=conv_name_base + '2a')(input_feature)\n",
    "        else:\n",
    "            x = residual_unit(filters=filters, kernel_size=(1, 1),\n",
    "                              strides=transition_strides,\n",
    "                              dilation_rate=dilation_rate,\n",
    "                              conv_name_base=conv_name_base + '2a',\n",
    "                              bn_name_base=bn_name_base + '2a')(input_feature)\n",
    "\n",
    "        if dropout is not None:\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "        x = residual_unit(filters=filters, kernel_size=(3, 3),\n",
    "                          conv_name_base=conv_name_base + '2b',\n",
    "                          bn_name_base=bn_name_base + '2b')(x)\n",
    "\n",
    "        if dropout is not None:\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "        x = residual_unit(filters=filters * 4, kernel_size=(1, 1),\n",
    "                          conv_name_base=conv_name_base + '2c',\n",
    "                          bn_name_base=bn_name_base + '2c')(x)\n",
    "\n",
    "        return _shortcut(input_feature, x)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _string_to_function(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "\n",
    "def ResNet(input_shape=None, classes=10, block='bottleneck', residual_unit='v2',\n",
    "           repetitions=None, initial_filters=128, activation= 'softmax', include_top=True,\n",
    "           input_tensor=None, dropout=None, transition_dilation_rate=(1, 1),\n",
    "           initial_strides=(1, 1), initial_kernel_size=(7, 7), initial_pooling='none',\n",
    "           final_pooling='max', top='classification'):\n",
    "    \"\"\"Builds a custom ResNet like architecture. Defaults to ResNet50 v2.\n",
    "    Args:\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` dim ordering)\n",
    "            or `(3, 224, 224)` (with `channels_first` dim ordering).\n",
    "            It should have exactly 3 dimensions,\n",
    "            and width and height should be no smaller than 8.\n",
    "            E.g. `(224, 224, 3)` would be one valid value.\n",
    "        classes: The number of outputs at final softmax layer\n",
    "        block: The block function to use. This is either `'basic'` or `'bottleneck'`.\n",
    "            The original paper used `basic` for layers < 50.\n",
    "        repetitions: Number of repetitions of various block units.\n",
    "            At each block unit, the number of filters are doubled and the input size\n",
    "            is halved. Default of None implies the ResNet50v2 values of [3, 4, 6, 3].\n",
    "        residual_unit: the basic residual unit, 'v1' for conv bn relu, 'v2' for bn relu\n",
    "            conv. See [Identity Mappings in\n",
    "            Deep Residual Networks](https://arxiv.org/abs/1603.05027)\n",
    "            for details.\n",
    "        dropout: None for no dropout, otherwise rate of dropout from 0 to 1.\n",
    "            Based on [Wide Residual Networks.(https://arxiv.org/pdf/1605.07146) paper.\n",
    "        transition_dilation_rate: Dilation rate for transition layers. For semantic\n",
    "            segmentation of images use a dilation rate of (2, 2).\n",
    "        initial_strides: Stride of the very first residual unit and MaxPooling2D call,\n",
    "            with default (2, 2), set to (1, 1) for small images like cifar.\n",
    "        initial_kernel_size: kernel size of the very first convolution, (7, 7) for\n",
    "            imagenet and (3, 3) for small image datasets like tiny imagenet and cifar.\n",
    "            See [ResNeXt](https://arxiv.org/abs/1611.05431) paper for details.\n",
    "        initial_pooling: Determine if there will be an initial pooling layer,\n",
    "            'max' for imagenet and None for small image datasets.\n",
    "            See [ResNeXt](https://arxiv.org/abs/1611.05431) paper for details.\n",
    "        final_pooling: Optional pooling mode for feature extraction at the final\n",
    "            model layer when `include_top` is `False`.\n",
    "            - `None` means that the output of the model\n",
    "                will be the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a\n",
    "                2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        top: Defines final layers to evaluate based on a specific problem type. Options\n",
    "            are 'classification' for ImageNet style problems, 'segmentation' for\n",
    "            problems like the Pascal VOC dataset, and None to exclude these layers\n",
    "            entirely.\n",
    "    Returns:\n",
    "        The keras `Model`.\n",
    "    \"\"\"\n",
    "    if activation not in ['softmax', 'sigmoid', None]:\n",
    "        raise ValueError('activation must be one of \"softmax\", \"sigmoid\", or None')\n",
    "    if activation == 'sigmoid' and classes != 1:\n",
    "        raise ValueError('sigmoid activation can only be used when classes = 1')\n",
    "    if repetitions is None:\n",
    "        repetitions = [3, 4, 6, 3]\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=32,\n",
    "                                      min_size=8,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "    _handle_dim_ordering()\n",
    "    if len(input_shape) != 3:\n",
    "        raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "    if block == 'basic':\n",
    "        block_fn = basic_block\n",
    "    elif block == 'bottleneck':\n",
    "        block_fn = bottleneck\n",
    "    elif isinstance(block, six.string_types):\n",
    "        block_fn = _string_to_function(block)\n",
    "    else:\n",
    "        block_fn = block\n",
    "\n",
    "    if residual_unit == 'v2':\n",
    "        residual_unit = _bn_relu_conv\n",
    "    elif residual_unit == 'v1':\n",
    "        residual_unit = _conv_bn_relu\n",
    "    elif isinstance(residual_unit, six.string_types):\n",
    "        residual_unit = _string_to_function(residual_unit)\n",
    "    else:\n",
    "        residual_unit = residual_unit\n",
    "\n",
    "    # Permute dimension order if necessary\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=32,\n",
    "                                      min_size=8,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    img_input = Input(shape=input_shape, tensor=input_tensor)\n",
    "    x = _conv_bn_relu(filters=initial_filters, kernel_size=initial_kernel_size,\n",
    "                      strides=initial_strides)(img_input)\n",
    "    if initial_pooling == 'max':\n",
    "        x = MaxPooling2D(pool_size=(3, 3), strides=initial_strides, padding=\"same\")(x)\n",
    "\n",
    "    block = x\n",
    "    filters = initial_filters\n",
    "    for i, r in enumerate(repetitions):\n",
    "        transition_dilation_rates = [transition_dilation_rate] * r\n",
    "        transition_strides = [(1, 1)] * r\n",
    "        if transition_dilation_rate == (1, 1):\n",
    "            transition_strides[0] = (2, 2)\n",
    "        block = _residual_block(block_fn, filters=filters,\n",
    "                                stage=i, blocks=r,\n",
    "                                is_first_layer=(i == 0),\n",
    "                                dropout=dropout,\n",
    "                                transition_dilation_rates=transition_dilation_rates,\n",
    "                                transition_strides=transition_strides,\n",
    "                                residual_unit=residual_unit)(block)\n",
    "        filters *= 2\n",
    "\n",
    "    # Last activation\n",
    "    x = _bn_relu(block)\n",
    "\n",
    "    # Classifier block\n",
    "    if include_top and top is 'classification':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(units=classes, activation=activation,\n",
    "                  kernel_initializer=\"he_normal\")(x)\n",
    "    elif include_top and top is 'segmentation':\n",
    "        x = SeparableConv2D(classes, (1, 1), activation='linear', padding='same')(x)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            channel, row, col = input_shape\n",
    "        else:\n",
    "            row, col, channel = input_shape\n",
    "\n",
    "        x = Reshape((row * col, classes))(x)\n",
    "        x = Activation(activation)(x)\n",
    "        x = Reshape((row, col, classes))(x)\n",
    "    elif final_pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif final_pooling == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    model = Model(inputs=img_input, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet18(input_shape, classes):\n",
    "    \"\"\"ResNet with 18 layers and v2 residual units\n",
    "    \"\"\"\n",
    "    return ResNet(input_shape, classes, basic_block, repetitions=[2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34(input_shape, classes):\n",
    "    \"\"\"ResNet with 34 layers and v2 residual units\n",
    "    \"\"\"\n",
    "    return ResNet(input_shape, classes, basic_block, repetitions=[3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50(input_shape, classes):\n",
    "    \"\"\"ResNet with 50 layers and v2 residual units\n",
    "    \"\"\"\n",
    "    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101(input_shape, classes):\n",
    "    \"\"\"ResNet with 101 layers and v2 residual units\n",
    "    \"\"\"\n",
    "    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152(input_shape, classes):\n",
    "    \"\"\"ResNet with 152 layers and v2 residual units\n",
    "    \"\"\"\n",
    "    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128  # orig paper trained all networks with batch_size= 128\n",
    "epochs = 50\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "pixel_level = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 45:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 40:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 30:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 10:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_eraser(p=0.2, s_l=0.02, s_h=0.5, r_1=0.2, r_2=1/0.2, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1206 23:13:23.979402  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1206 23:13:23.991369  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1206 23:13:23.993390  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W1206 23:13:24.021288  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1206 23:13:24.021288  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1206 23:13:25.014374  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 23:13:26.958141  9428 deprecation_wrapper.py:119] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 128)  18944       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "res0a_branch2a (SeparableConv2D (None, 16, 16, 128)  17664       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         res0a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 128)  16640       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 128)  17664       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
      "                                                                 separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 16, 16, 128)  17664       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 16, 16, 128)  17664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 128)  0           add_1[0][0]                      \n",
      "                                                                 separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 8, 8, 256)    34176       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 256)    1024        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 8, 8, 256)    33152       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 256)    68096       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "                                                                 separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 8, 8, 256)    68096       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 8, 8, 256)    68096       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 256)    0           add_3[0][0]                      \n",
      "                                                                 separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 256)    1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 4, 4, 512)    133888      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 4, 4, 512)    131840      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 4, 4, 512)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 4, 4, 512)    267264      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 512)    0           batch_normalization_13[0][0]     \n",
      "                                                                 separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 4, 4, 512)    267264      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 4, 4, 512)    267264      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 512)    0           add_5[0][0]                      \n",
      "                                                                 separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 512)    2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 512)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 2, 2, 1024)   529920      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2, 2, 1024)   4096        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 2, 2, 1024)   525824      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2, 2, 1024)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 2, 2, 1024)   4096        separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 2, 2, 1024)   1058816     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2, 2, 1024)   0           batch_normalization_18[0][0]     \n",
      "                                                                 separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 2, 2, 1024)   4096        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 2, 2, 1024)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 2, 2, 1024)   1058816     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 2, 2, 1024)   4096        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2, 2, 1024)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 2, 2, 1024)   1058816     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 1024)   0           add_7[0][0]                      \n",
      "                                                                 separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 2, 2, 1024)   4096        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2, 2, 1024)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1024)         0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           10250       global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 5,726,730\n",
      "Trainable params: 5,707,274\n",
      "Non-trainable params: 19,456\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18(input_shape=input_shape,classes = 10)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model model saving directory.\n",
    "model_type='ResNet18'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 23:13:27.526654  9428 deprecation.py:323] From C:\\Users\\aadur\\Anaconda3\\envs\\TF_GPU\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 41s 104ms/step - loss: 1.5741 - acc: 0.4383 - val_loss: 1.4594 - val_acc: 0.5048\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50480, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.001.h5\n",
      "Epoch 2/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 34s 88ms/step - loss: 1.1955 - acc: 0.5795 - val_loss: 1.2843 - val_acc: 0.5899\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50480 to 0.58990, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.002.h5\n",
      "Epoch 3/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 34s 88ms/step - loss: 0.9972 - acc: 0.6529 - val_loss: 1.0256 - val_acc: 0.6513\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58990 to 0.65130, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.003.h5\n",
      "Epoch 4/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 88ms/step - loss: 0.8768 - acc: 0.6990 - val_loss: 0.9579 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65130 to 0.68220, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.004.h5\n",
      "Epoch 5/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.7848 - acc: 0.7291 - val_loss: 0.8276 - val_acc: 0.7247\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.68220 to 0.72470, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.005.h5\n",
      "Epoch 6/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.7140 - acc: 0.7576 - val_loss: 0.9154 - val_acc: 0.7065\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72470\n",
      "Epoch 7/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 88ms/step - loss: 0.6670 - acc: 0.7717 - val_loss: 0.7316 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72470 to 0.75780, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.007.h5\n",
      "Epoch 8/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 88ms/step - loss: 0.6163 - acc: 0.7893 - val_loss: 0.6686 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.75780 to 0.78190, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.008.h5\n",
      "Epoch 9/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.5740 - acc: 0.8036 - val_loss: 0.7550 - val_acc: 0.7613\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78190\n",
      "Epoch 10/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.5390 - acc: 0.8171 - val_loss: 0.6050 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.78190 to 0.80300, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.010.h5\n",
      "Epoch 11/50\n",
      "Learning rate:  0.001\n",
      "391/390 [==============================] - 35s 88ms/step - loss: 0.5111 - acc: 0.8291 - val_loss: 0.6072 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.80300 to 0.80770, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.011.h5\n",
      "Epoch 12/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.4022 - acc: 0.8665 - val_loss: 0.4762 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.80770 to 0.84250, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.012.h5\n",
      "Epoch 13/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.3656 - acc: 0.8794 - val_loss: 0.4788 - val_acc: 0.8434\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.84250 to 0.84340, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.013.h5\n",
      "Epoch 14/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 34s 88ms/step - loss: 0.3596 - acc: 0.8816 - val_loss: 0.4754 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.84340 to 0.84450, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.014.h5\n",
      "Epoch 15/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.3403 - acc: 0.8883 - val_loss: 0.4625 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84450 to 0.85020, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.015.h5\n",
      "Epoch 16/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.3289 - acc: 0.8927 - val_loss: 0.4620 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85020 to 0.85170, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.016.h5\n",
      "Epoch 17/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 88ms/step - loss: 0.3239 - acc: 0.8925 - val_loss: 0.4735 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85170\n",
      "Epoch 18/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.3112 - acc: 0.8968 - val_loss: 0.4750 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85170\n",
      "Epoch 19/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.3052 - acc: 0.8997 - val_loss: 0.4882 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85170\n",
      "Epoch 20/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2974 - acc: 0.9039 - val_loss: 0.4707 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85170\n",
      "Epoch 21/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2904 - acc: 0.9046 - val_loss: 0.4652 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85170 to 0.85400, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.021.h5\n",
      "Epoch 22/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2834 - acc: 0.9078 - val_loss: 0.4716 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85400 to 0.85470, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.022.h5\n",
      "Epoch 23/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2819 - acc: 0.9067 - val_loss: 0.4741 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85470\n",
      "Epoch 24/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2711 - acc: 0.9122 - val_loss: 0.4731 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85470\n",
      "Epoch 25/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2654 - acc: 0.9138 - val_loss: 0.4783 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.85470\n",
      "Epoch 26/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2595 - acc: 0.9142 - val_loss: 0.4749 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85470\n",
      "Epoch 27/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2527 - acc: 0.9184 - val_loss: 0.4674 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.85470 to 0.85770, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.027.h5\n",
      "Epoch 28/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2467 - acc: 0.9202 - val_loss: 0.4710 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.85770\n",
      "Epoch 29/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2427 - acc: 0.9204 - val_loss: 0.4687 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.85770\n",
      "Epoch 30/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2364 - acc: 0.9237 - val_loss: 0.4870 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.85770\n",
      "Epoch 31/50\n",
      "Learning rate:  0.0001\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2370 - acc: 0.9229 - val_loss: 0.4774 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85770\n",
      "Epoch 32/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2203 - acc: 0.9296 - val_loss: 0.4708 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85770\n",
      "Epoch 33/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2167 - acc: 0.9301 - val_loss: 0.4697 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.85770 to 0.85860, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.033.h5\n",
      "Epoch 34/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2166 - acc: 0.9318 - val_loss: 0.4667 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.85860 to 0.85960, saving model to C:\\Users\\aadur\\saved_models\\cifar10_ResNet18_model.034.h5\n",
      "Epoch 35/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2144 - acc: 0.9326 - val_loss: 0.4669 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85960\n",
      "Epoch 36/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2164 - acc: 0.9319 - val_loss: 0.4680 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85960\n",
      "Epoch 37/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2128 - acc: 0.9319 - val_loss: 0.4693 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85960\n",
      "Epoch 38/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2101 - acc: 0.9337 - val_loss: 0.4672 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85960\n",
      "Epoch 39/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2101 - acc: 0.9339 - val_loss: 0.4714 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85960\n",
      "Epoch 40/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2142 - acc: 0.9321 - val_loss: 0.4694 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85960\n",
      "Epoch 41/50\n",
      "Learning rate:  1e-05\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2067 - acc: 0.9348 - val_loss: 0.4691 - val_acc: 0.8574\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85960\n",
      "Epoch 42/50\n",
      "Learning rate:  1e-06\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2055 - acc: 0.9355 - val_loss: 0.4682 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85960\n",
      "Epoch 43/50\n",
      "Learning rate:  1e-06\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2061 - acc: 0.9349 - val_loss: 0.4667 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85960\n",
      "Epoch 44/50\n",
      "Learning rate:  1e-06\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2057 - acc: 0.9354 - val_loss: 0.4667 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85960\n",
      "Epoch 45/50\n",
      "Learning rate:  1e-06\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2063 - acc: 0.9351 - val_loss: 0.4680 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85960\n",
      "Epoch 46/50\n",
      "Learning rate:  1e-06\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2054 - acc: 0.9351 - val_loss: 0.4676 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.85960\n",
      "Epoch 47/50\n",
      "Learning rate:  5e-07\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2070 - acc: 0.9352 - val_loss: 0.4693 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.85960\n",
      "Epoch 48/50\n",
      "Learning rate:  5e-07\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2041 - acc: 0.9357 - val_loss: 0.4684 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.85960\n",
      "Epoch 49/50\n",
      "Learning rate:  5e-07\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2063 - acc: 0.9350 - val_loss: 0.4683 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.85960\n",
      "Epoch 50/50\n",
      "Learning rate:  5e-07\n",
      "391/390 [==============================] - 35s 89ms/step - loss: 0.2058 - acc: 0.9342 - val_loss: 0.4690 - val_acc: 0.8574\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.85960\n"
     ]
    }
   ],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=True,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=True,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale= None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level),\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format= None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,steps_per_epoch=len(x_train)/batch_size,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 262us/step\n",
      "Test loss: 0.4689581654548645\n",
      "Test accuracy: 0.8574\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
